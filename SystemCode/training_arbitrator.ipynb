{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "STVzL9mAaoMa"
   },
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions of key libraries\n",
      "---\n",
      "tensorflow:  2.9.0\n",
      "numpy:       1.22.3\n",
      "matplotlib:  3.5.2\n",
      "sklearn:     1.1.1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Versions of key libraries\")\n",
    "print(\"---\")\n",
    "print(\"tensorflow: \", tf.__version__)\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25812,
     "status": "ok",
     "timestamp": 1653205963445,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "6NqhUlrAaoMb",
    "outputId": "a3b1e3e9-d797-426a-dee9-136eb6d623f7"
   },
   "outputs": [],
   "source": [
    "members_param = [\n",
    "    {\n",
    "        \"model_name\": \"general\",\n",
    "        \"file_path\": \"./models/medium_224_20_64_1_2022-09-25_19-33-18.hdf5\",\n",
    "        \"class_names\": ['arowana', 'betta', 'goldfish', 'flowerhorn'],\n",
    "        \"img_width\": 224,\n",
    "        \"img_height\":160\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"arawana\",\n",
    "        \"file_path\": \"./models/arowana_softmax_128_50_64_1_2022-09-25_22-34-38.hdf5\",\n",
    "        \"class_names\": ['not arowana', 'arowana'],\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"betta\",\n",
    "        \"file_path\": \"./models/betta_softmax_128_50_64_1_2022-09-26_17-51-39.hdf5\",\n",
    "        \"class_names\": ['not betta', 'betta'],\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"goldfish\",\n",
    "        \"file_path\": \"./models/goldfish_softmax_128_50_64_1_2022-09-25_22-37-40.hdf5\",\n",
    "        \"class_names\": ['not goldfish', 'goldfish'],\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"flowerhorn\",\n",
    "        \"file_path\": \"./models/luohan_softmax_128_50_64_1_2022-09-25_22-42-17.hdf5\",\n",
    "        \"class_names\": ['not flowerhorn', 'flowerhorn'],\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    }\n",
    "]\n",
    "\n",
    "modelname   = 'arbitrator'\n",
    "BATCH_SIZE = 32 # Big enough to measure an F1-score\n",
    "AUTOTUNE = tf.data.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 160\n",
    "CHANNELS = 3\n",
    "SEED = 7\n",
    "N_LABELS = 4\n",
    "EPOCHS = 5\n",
    "OPT_IDX = 1\n",
    "ACTIVATION = 'softmax'\n",
    "\n",
    "class_names = ''\n",
    "modelname = modelname+\"_\"+ACTIVATION+\"_\"+str(EPOCHS)+\"_\"+str(BATCH_SIZE)+\"_\"+str(OPT_IDX)\n",
    "optmzs = ['adam', optimizers.RMSprop(learning_rate=0.0001)]\n",
    "optmz = optmzs[OPT_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2873,
     "status": "ok",
     "timestamp": 1653205981198,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "ODxtvchaaoMc",
    "outputId": "7a09613d-babb-43c8-cee7-065c71792ce0"
   },
   "outputs": [],
   "source": [
    "def readImagesFromDir(base_img_path='dataset/'):\n",
    "    dirs = [d for d in listdir(base_img_path) if isdir(join(base_img_path, d)) and not d.startswith('.') and not d in ['oranda', 'common_goldfish']]\n",
    "\n",
    "    print(dirs)\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for d in dirs:\n",
    "        img_path = base_img_path + d + \"/\"\n",
    "        files = [f for f in listdir(img_path) if isfile(join(img_path, f))]\n",
    "        X = X + [os.path.join(img_path, f) for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        y = y + [d for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        #print(d)\n",
    "\n",
    "    data_dir = Path(base_img_path)\n",
    "    image_count = len(list(data_dir.glob('*/*.*')))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def preprocess_image(filename, label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    # Read an image from a file\n",
    "    images = {}\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "#     image_resized = tf.image.resize_with_pad(image_decoded, IMG_HEIGHT, IMG_WIDTH, antialias=False)\n",
    "#     image_normalized = image_resized / 255.0\n",
    "    for i,m in enumerate(members_param):\n",
    "        image_resized = tf.image.resize_with_pad(image_decoded, m[\"img_height\"], m[\"img_width\"], antialias=False)\n",
    "        image_normalized = image_resized / 255.0\n",
    "        images[\"input_\"+str(i)] = image_normalized\n",
    "        \n",
    "    return images, label\n",
    "\n",
    "\n",
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    \n",
    "#     imageDS = \n",
    "    \n",
    "#     labelDS = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if is_training == True:\n",
    "        #dataset = dataset.take(BATCH_SIZE)\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        #dataset = dataset.cache()\n",
    "        #dataset = dataset.repeat()\n",
    "        # Shuffle the data each buffer size\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "        #dataset = dataset.repeat()\n",
    "        \n",
    "    # Batch the data for multiple steps    \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def visualize_image(original, augmented):\n",
    "    org_img = tf.keras.utils.array_to_img(original)\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img)\n",
    "\n",
    "    aug_img = tf.keras.utils.array_to_img(augmented)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Augmented image')\n",
    "    plt.imshow(aug_img)\n",
    "\n",
    "\n",
    "def predict_new_image(img_file):    \n",
    "    #img = tf.keras.utils.load_img(\n",
    "    #    img_file, target_size=(IMG_HEIGHT, IMG_WIDTH), keep_aspect_ratio=True\n",
    "    #)\n",
    "    img = tf.keras.utils.load_img(\n",
    "        img_file, target_size=None, keep_aspect_ratio=True\n",
    "    )\n",
    "\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.keras.preprocessing.image.smart_resize(img_array, size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    image_normalized = img_array / 255.0  # tf.image.per_image_standardization(img_array)\n",
    "\n",
    "    saved_model = load_model(model_file)\n",
    "\n",
    "    predictions = model.predict(image_normalized)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "    #score = tf.sigmoid(predictions[0])\n",
    "    #score = tf.nn.sigmoid_cross_entropy_with_logits(labels=class_names, logits=predictions[0])\n",
    "    #score = tf.math.sigmoid(predictions[0])\n",
    "    #score = tf.tanh(predictions[0])\n",
    "\n",
    "    #model.evaluate(img_array)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    #print(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arowana', 'betta', 'goldfish', 'luohan']\n",
      "0. arowana\n",
      "1. betta\n",
      "2. goldfish\n",
      "3. luohan\n",
      "(5872, 4)\n",
      "(1469, 4)\n"
     ]
    }
   ],
   "source": [
    "X, Y = readImagesFromDir()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=SEED)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "class_names = lb.classes_\n",
    "# Loop over all labels and show them    \n",
    "N_LABELS = len(class_names)\n",
    "for (i, label) in enumerate(class_names):\n",
    "    print(\"{}. {}\".format(i, label))\n",
    "\n",
    "# transform the targets of the training and test sets\n",
    "y_train_bin = lb.transform(y_train)\n",
    "y_val_bin = lb.transform(y_val)\n",
    "\n",
    "print(y_train_bin.shape)\n",
    "print(y_val_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/goldfish/Goldfish2293.jpg [0 0 1 0]\n",
      "dataset/arowana/Arowana_101220.jpg [1 0 0 0]\n",
      "dataset/arowana/Arowana_100537.jpg [1 0 0 0]\n",
      "dataset/betta/betta0551.jpg [0 1 0 0]\n",
      "dataset/luohan/Flowerhorn10533.jpg [0 0 0 1]\n",
      "dataset/goldfish/Goldfish2329.jpg [0 0 1 0]\n",
      "dataset/arowana/Arowana_100919.jpg [1 0 0 0]\n",
      "dataset/luohan/Flowerhorn10569.jpg [0 0 0 1]\n",
      "dataset/arowana/Arowana_101372.jpg [1 0 0 0]\n",
      "dataset/goldfish/Goldfish1244.jpg [0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Print examples of fish and their binary targets\n",
    "for i in range(10):\n",
    "    print(X_train[len(X_train)-1 - i], y_train_bin[len(y_train_bin)-1 - i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = []\n",
    "# val_ds = []\n",
    "\n",
    "train_ds = create_dataset(X_train, y_train_bin)\n",
    "val_ds = create_dataset(X_val, y_val_bin, is_training=False)\n",
    "\n",
    "# for images, labels in train_ds.take(1):\n",
    "#     print(\"Shape of features array:\", images.numpy().shape)\n",
    "#     print(\"Shape of labels array:\", labels.numpy().shape)\n",
    "#     #plt.imshow(f.numpy().astype(\"uint8\"))\n",
    "#     for i in range(5):\n",
    "#         ax = plt.subplot(2, 3, i + 1)\n",
    "#         img = tf.keras.utils.array_to_img(images[i])\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(class_names[np.argmax(labels[i])])\n",
    "#         plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8122,
     "status": "ok",
     "timestamp": 1653206045624,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "gRdnqYDSaoMc",
    "outputId": "417b2730-29c8-4554-b302-5715ef394613"
   },
   "outputs": [],
   "source": [
    "def add_prefix(model, prefix: str, i:str, custom_objects=None):\n",
    "    config = model.get_config()\n",
    "    new_to_old = {}\n",
    "    for layer in config['layers']:\n",
    "#         print(layer)\n",
    "        new_name = prefix + i + layer['config']['name']\n",
    "        if layer['class_name']=='InputLayer':\n",
    "            new_name = \"input_\"+i\n",
    "        new_to_old[new_name] = layer['config']['name']\n",
    "#         layer['name'] = new_name\n",
    "        layer['config']['name'] = new_name\n",
    "    if config['name'].startswith('sequential'):\n",
    "        new_model = tf.keras.Sequential().from_config(config, custom_objects)\n",
    "    else:\n",
    "        new_model = tf.keras.Model().from_config(config, custom_objects)\n",
    "    for layer in new_model.layers:\n",
    "        layer.set_weights(model.get_layer(new_to_old[layer.name]).get_weights())\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(members_param):\n",
    "    all_models = list()\n",
    "    prefix = \"ensemble_\"\n",
    "    \n",
    "    for i, m in enumerate(members_param):\n",
    "        # define filename for this ensemble\n",
    "        filename = m[\"file_path\"]\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(add_prefix(model,prefix,str(i)))\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    " \n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(12, activation='relu')(merge)\n",
    "    output = Dense(N_LABELS, activation=ACTIVATION)(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optmz, metrics=['accuracy'])\n",
    "    return model\n",
    "member_models = load_all_models(members_param)\n",
    "model = define_stacked_model(member_models)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1653206059750,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "oiFHNVtKaoMc",
    "outputId": "548eef54-ba28-4e79-f3d5-9e156fc04d89"
   },
   "outputs": [],
   "source": [
    "                                                                                # Step 1\n",
    "modelname       = modelname+\"_\"+str(datetime.datetime.now())[:-7].replace(' ','_').replace(\":\",'-')\n",
    "folderpath      = 'models/'\n",
    "model_json      = folderpath + modelname + \".json\"\n",
    "with open(model_json, \"w\") as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "model_file      = folderpath + modelname + \".hdf5\"\n",
    "checkpoint      = ModelCheckpoint(model_file, \n",
    "                                  monitor='val_accuracy', \n",
    "                                  verbose=0, \n",
    "                                  save_best_only=True, \n",
    "                                  mode='max')\n",
    "\n",
    "csv_logger      = CSVLogger(folderpath+modelname +'.csv')                       # Step 2\n",
    "callbacks_list  = [checkpoint,csv_logger]                                       # Step 3\n",
    "\n",
    "print(\"Path to model:\", model_file)\n",
    "print(\"Path to log:  \", folderpath+modelname+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as time\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(train_ds,                            # Training data and label\n",
    "          validation_data=val_ds,   # Validation data and label\n",
    "          epochs=EPOCHS,                       # The amount of epochs to be trained\n",
    "          batch_size=BATCH_SIZE,                   \n",
    "          shuffle=True,                     # To shuffle the training data\n",
    "          callbacks=callbacks_list)         # Callbacks to execute the checkpoints\n",
    "\n",
    "end = time.time()\n",
    "duration = round(((end - start)/60), 2)\n",
    "print(\"duration = \", duration, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSN4w8WeaoMd"
   },
   "outputs": [],
   "source": [
    "plotpath  = folderpath+modelname+'_plot.png'\n",
    "plot_model(model, \n",
    "           to_file=plotpath, \n",
    "           show_shapes=True, \n",
    "           show_layer_names=False,\n",
    "           rankdir='TB')\n",
    "print(\"Path to plot:\", plotpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                # Step 1\n",
    "modelGo = load_model(model_file)\n",
    "\n",
    "predicts    = modelGo.predict(val_ds)                                            # Step 2\n",
    "print(\"Prediction completes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                # Step 1\n",
    "                                                                                # Step 2\n",
    "predout     = np.argmax(predicts,axis=1)\n",
    "testout     = np.argmax(y_val_bin,axis=1)\n",
    "\n",
    "testScores  = metrics.accuracy_score(testout,predout)                           # Step 3\n",
    "\n",
    "                                                                                # Step 4\n",
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100))\n",
    "print(metrics.classification_report(testout,\n",
    "                                    predout,\n",
    "                                    target_names=class_names,\n",
    "                                    digits=4))\n",
    "\n",
    "report = metrics.classification_report(testout,\n",
    "                                    predout,\n",
    "                                    target_names=class_names,\n",
    "                                    digits=4,\n",
    "                                      output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(folderpath+modelname+'_report.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf = pd.DataFrame()\n",
    "resdf[\"file\"] = X_val\n",
    "resdf[\"fish\"] = y_val\n",
    "resdf[\"testout\"] = testout\n",
    "resdf[\"predout\"] = predout\n",
    "print(resdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print all wrong predition\n",
    "resdf[resdf.testout !=resdf.predout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "D5 StackingEnsembleNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
