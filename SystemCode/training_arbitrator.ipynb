{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STVzL9mAaoMa"
   },
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions of key libraries\n",
      "---\n",
      "tensorflow:  2.9.0\n",
      "numpy:       1.22.3\n",
      "matplotlib:  3.5.2\n",
      "sklearn:     1.1.1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Versions of key libraries\")\n",
    "print(\"---\")\n",
    "print(\"tensorflow: \", tf.__version__)\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25812,
     "status": "ok",
     "timestamp": 1653205963445,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "6NqhUlrAaoMb",
    "outputId": "a3b1e3e9-d797-426a-dee9-136eb6d623f7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "members_param = [\n",
    "    {\n",
    "        \"fishType\": \"general\",\n",
    "        \"filePath\": \"./models/medium_224_20_64_1_2022-09-25_19-33-18.hdf5\",\n",
    "        \"img_width\": 224,\n",
    "        \"img_height\":160\n",
    "    },\n",
    "    {\n",
    "        \"fishType\": \"arawana\",\n",
    "        \"filePath\": \"./models/arowana_softmax_128_50_64_1_2022-09-25_22-34-38.hdf5\",\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"fishType\": \"betta\",\n",
    "        \"filePath\": \"./models/betta_softmax_128_50_64_1_2022-09-26_17-51-39.hdf5\",\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"fishType\": \"goldfish\",\n",
    "        \"filePath\": \"./models/goldfish_softmax_128_50_64_1_2022-09-25_22-37-40.hdf5\",\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"fishType\": \"flowerhorn\",\n",
    "        \"filePath\": \"./models/luohan_softmax_128_50_64_1_2022-09-25_22-42-17.hdf5\",\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    }\n",
    "]\n",
    "\n",
    "modelname   = 'arbitrator'\n",
    "BATCH_SIZE = 32 # Big enough to measure an F1-score\n",
    "AUTOTUNE = tf.data.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 160\n",
    "CHANNELS = 3\n",
    "SEED = 7\n",
    "N_LABELS = 4\n",
    "EPOCHS = 50\n",
    "OPT_IDX = 1\n",
    "ACTIVATION = 'softmax'\n",
    "\n",
    "class_names = ''\n",
    "modelname = modelname+\"_\"+ACTIVATION+\"_\"+str(EPOCHS)+\"_\"+str(BATCH_SIZE)+\"_\"+str(OPT_IDX)\n",
    "optmzs = ['adam', optimizers.RMSprop(learning_rate=0.0001)]\n",
    "optmz = optmzs[OPT_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2873,
     "status": "ok",
     "timestamp": 1653205981198,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "ODxtvchaaoMc",
    "outputId": "7a09613d-babb-43c8-cee7-065c71792ce0"
   },
   "outputs": [],
   "source": [
    "def readImagesFromDir(base_img_path='dataset/'):\n",
    "    dirs = [d for d in listdir(base_img_path) if isdir(join(base_img_path, d)) and not d.startswith('.') and not d in ['oranda', 'common_goldfish']]\n",
    "\n",
    "    print(dirs)\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for d in dirs:\n",
    "        img_path = base_img_path + d + \"/\"\n",
    "        files = [f for f in listdir(img_path) if isfile(join(img_path, f))]\n",
    "        X = X + [os.path.join(img_path, f) for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        y = y + [d for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        #print(d)\n",
    "\n",
    "    data_dir = Path(base_img_path)\n",
    "    image_count = len(list(data_dir.glob('*/*.*')))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def preprocess_image(filename, label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    # Read an image from a file\n",
    "    images = {}\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "#     image_resized = tf.image.resize_with_pad(image_decoded, IMG_HEIGHT, IMG_WIDTH, antialias=False)\n",
    "#     image_normalized = image_resized / 255.0\n",
    "    for i,m in enumerate(members_param):\n",
    "        image_resized = tf.image.resize_with_pad(image_decoded, m[\"img_height\"], m[\"img_width\"], antialias=False)\n",
    "        image_normalized = image_resized / 255.0\n",
    "        images[\"input_\"+str(i)] = image_normalized\n",
    "        \n",
    "    return images, label\n",
    "\n",
    "\n",
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    \n",
    "#     imageDS = \n",
    "    \n",
    "#     labelDS = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if is_training == True:\n",
    "        #dataset = dataset.take(BATCH_SIZE)\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        #dataset = dataset.cache()\n",
    "        #dataset = dataset.repeat()\n",
    "        # Shuffle the data each buffer size\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "        #dataset = dataset.repeat()\n",
    "        \n",
    "    # Batch the data for multiple steps    \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def visualize_image(original, augmented):\n",
    "    org_img = tf.keras.utils.array_to_img(original)\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img)\n",
    "\n",
    "    aug_img = tf.keras.utils.array_to_img(augmented)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Augmented image')\n",
    "    plt.imshow(aug_img)\n",
    "\n",
    "\n",
    "def predict_new_image(img_file):    \n",
    "    #img = tf.keras.utils.load_img(\n",
    "    #    img_file, target_size=(IMG_HEIGHT, IMG_WIDTH), keep_aspect_ratio=True\n",
    "    #)\n",
    "    img = tf.keras.utils.load_img(\n",
    "        img_file, target_size=None, keep_aspect_ratio=True\n",
    "    )\n",
    "\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.keras.preprocessing.image.smart_resize(img_array, size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    image_normalized = img_array / 255.0  # tf.image.per_image_standardization(img_array)\n",
    "\n",
    "    saved_model = load_model(model_file)\n",
    "\n",
    "    predictions = model.predict(image_normalized)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "    #score = tf.sigmoid(predictions[0])\n",
    "    #score = tf.nn.sigmoid_cross_entropy_with_logits(labels=class_names, logits=predictions[0])\n",
    "    #score = tf.math.sigmoid(predictions[0])\n",
    "    #score = tf.tanh(predictions[0])\n",
    "\n",
    "    #model.evaluate(img_array)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    #print(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arowana', 'betta', 'goldfish', 'luohan']\n",
      "0. arowana\n",
      "1. betta\n",
      "2. goldfish\n",
      "3. luohan\n",
      "(3297, 4)\n",
      "(825, 4)\n"
     ]
    }
   ],
   "source": [
    "X, Y = readImagesFromDir()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=SEED)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "class_names = lb.classes_\n",
    "# Loop over all labels and show them    \n",
    "N_LABELS = len(class_names)\n",
    "for (i, label) in enumerate(class_names):\n",
    "    print(\"{}. {}\".format(i, label))\n",
    "\n",
    "# transform the targets of the training and test sets\n",
    "y_train_bin = lb.transform(y_train)\n",
    "y_val_bin = lb.transform(y_val)\n",
    "\n",
    "print(y_train_bin.shape)\n",
    "print(y_val_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/betta/betta0194.jpg [0 1 0 0]\n",
      "dataset/arowana/Arrowana10533.jpg [1 0 0 0]\n",
      "dataset/goldfish/Goldfish1515.jpg [0 0 1 0]\n",
      "dataset/arowana/Arrowana10911.jpg [1 0 0 0]\n",
      "dataset/betta/betta0357.jpg [0 1 0 0]\n",
      "dataset/luohan/FHCichlid100114.jpg [0 0 0 1]\n",
      "dataset/goldfish/Goldfish1548.jpg [0 0 1 0]\n",
      "dataset/luohan/FHCichlid100525.jpg [0 0 0 1]\n",
      "dataset/goldfish/Goldfish1743.jpg [0 0 1 0]\n",
      "dataset/luohan/FHCichlid100096.jpg [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Print examples of fish and their binary targets\n",
    "for i in range(10):\n",
    "    print(X_train[len(X_train)-1 - i], y_train_bin[len(y_train_bin)-1 - i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = []\n",
    "# val_ds = []\n",
    "\n",
    "# for m in memebers_param:\n",
    "#     IMG_HEIGHT = m[\"img_height\"]\n",
    "#     IMG_WIDTH = m[\"img_width\"]\n",
    "#     train_ds.append(create_dataset(X_train, y_train_bin))\n",
    "#     val_ds.append(create_dataset(X_val, y_val_bin, is_training=False))\n",
    "\n",
    "train_ds = create_dataset(X_train, y_train_bin)\n",
    "val_ds = create_dataset(X_val, y_val_bin, is_training=False)\n",
    "\n",
    "# for images, labels in train_ds.take(1):\n",
    "#     print(\"Shape of features array:\", images.numpy().shape)\n",
    "#     print(\"Shape of labels array:\", labels.numpy().shape)\n",
    "#     #plt.imshow(f.numpy().astype(\"uint8\"))\n",
    "#     for i in range(5):\n",
    "#         ax = plt.subplot(2, 3, i + 1)\n",
    "#         img = tf.keras.utils.array_to_img(images[i])\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(class_names[np.argmax(labels[i])])\n",
    "#         plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8122,
     "status": "ok",
     "timestamp": 1653206045624,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "gRdnqYDSaoMc",
    "outputId": "417b2730-29c8-4554-b302-5715ef394613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded ./models/medium_224_20_64_1_2022-09-25_19-33-18.hdf5\n",
      ">loaded ./models/arowana_softmax_128_50_64_1_2022-09-25_22-34-38.hdf5\n",
      ">loaded ./models/betta_softmax_128_50_64_1_2022-09-26_17-51-39.hdf5\n",
      ">loaded ./models/goldfish_softmax_128_50_64_1_2022-09-25_22-37-40.hdf5\n",
      ">loaded ./models/luohan_softmax_128_50_64_1_2022-09-25_22-42-17.hdf5\n",
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_0 (InputLayer)           [(None, 160, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_495 (Conv2D)  (None, 160, 224, 32  2432        ['input_0[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_493 (M  (None, 80, 112, 32)  0          ['ensemble_0conv2d_495[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_496 (Conv2D)  (None, 80, 112, 64)  51264       ['ensemble_0max_pooling2d_493[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_494 (M  (None, 40, 56, 64)  0           ['ensemble_0conv2d_496[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_497 (Conv2D)  (None, 40, 56, 128)  204928      ['ensemble_0max_pooling2d_494[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_495 (M  (None, 20, 28, 128)  0          ['ensemble_0conv2d_497[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_498 (Conv2D)  (None, 20, 28, 256)  524544      ['ensemble_0max_pooling2d_495[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_496 (M  (None, 10, 14, 256)  0          ['ensemble_0conv2d_498[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_499 (Conv2D)  (None, 10, 14, 512)  2097664     ['ensemble_0max_pooling2d_496[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_497 (M  (None, 5, 7, 512)   0           ['ensemble_0conv2d_499[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_500 (Conv2D)  (None, 5, 7, 768)    3539712     ['ensemble_0max_pooling2d_497[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_498 (M  (None, 2, 3, 768)   0           ['ensemble_0conv2d_500[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0dropout_54 (Dropout)  (None, 2, 3, 768)   0           ['ensemble_0max_pooling2d_498[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " ensemble_0flatten_98 (Flatten)  (None, 4608)        0           ['ensemble_0dropout_54[0][0]']   \n",
      "                                                                                                  \n",
      " ensemble_1conv2d (Conv2D)      (None, 128, 128, 32  896         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_2conv2d (Conv2D)      (None, 128, 128, 32  896         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_3conv2d (Conv2D)      (None, 128, 128, 32  896         ['input_3[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_4conv2d (Conv2D)      (None, 128, 128, 32  896         ['input_4[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_0dense_572 (Dense)    (None, 768)          3539712     ['ensemble_0flatten_98[0][0]']   \n",
      "                                                                                                  \n",
      " ensemble_1max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_1conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_2max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_2conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_3max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_3conv2d[0][0]']       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_4max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_4conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_0dense_573 (Dense)    (None, 512)          393728      ['ensemble_0dense_572[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_1max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_2conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_2max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_3conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_3max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_4conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_4max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_0dense_574 (Dense)    (None, 256)          131328      ['ensemble_0dense_573[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_1conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_2max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_2conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_3max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_3conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_4max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_4conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_0dense_575 (Dense)    (None, 64)           16448       ['ensemble_0dense_574[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1dropout (Dropout)    (None, 32, 32, 16)   0           ['ensemble_1max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_2dropout (Dropout)    (None, 32, 32, 16)   0           ['ensemble_2max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_3dropout (Dropout)    (None, 32, 32, 16)   0           ['ensemble_3max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_4dropout (Dropout)    (None, 32, 32, 16)   0           ['ensemble_4max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_0dense_576 (Dense)    (None, 32)           2080        ['ensemble_0dense_575[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1flatten (Flatten)    (None, 16384)        0           ['ensemble_1dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_2flatten (Flatten)    (None, 16384)        0           ['ensemble_2dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_3flatten (Flatten)    (None, 16384)        0           ['ensemble_3dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_4flatten (Flatten)    (None, 16384)        0           ['ensemble_4dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_0dense_577 (Dense)    (None, 16)           528         ['ensemble_0dense_576[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1dense (Dense)        (None, 10)           163850      ['ensemble_1flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_2dense (Dense)        (None, 10)           163850      ['ensemble_2flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_3dense (Dense)        (None, 10)           163850      ['ensemble_3flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_4dense (Dense)        (None, 10)           163850      ['ensemble_4flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_0dense_578 (Dense)    (None, 4)            68          ['ensemble_0dense_577[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1dense_1 (Dense)      (None, 2)            22          ['ensemble_1dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_2dense_1 (Dense)      (None, 2)            22          ['ensemble_2dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_3dense_1 (Dense)      (None, 2)            22          ['ensemble_3dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_4dense_1 (Dense)      (None, 2)            22          ['ensemble_4dense[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_9 (Concatenate)    (None, 12)           0           ['ensemble_0dense_578[0][0]',    \n",
      "                                                                  'ensemble_1dense_1[0][0]',      \n",
      "                                                                  'ensemble_2dense_1[0][0]',      \n",
      "                                                                  'ensemble_3dense_1[0][0]',      \n",
      "                                                                  'ensemble_4dense_1[0][0]']      \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 12)           156         ['concatenate_9[0][0]']          \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 4)            52          ['dense_18[0][0]']               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Total params: 11,182,212\n",
      "Trainable params: 208\n",
      "Non-trainable params: 11,182,004\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def add_prefix(model, prefix: str, i:str, custom_objects=None):\n",
    "    config = model.get_config()\n",
    "    new_to_old = {}\n",
    "    for layer in config['layers']:\n",
    "#         print(layer)\n",
    "        new_name = prefix + i + layer['config']['name']\n",
    "        if layer['class_name']=='InputLayer':\n",
    "            new_name = \"input_\"+i\n",
    "        new_to_old[new_name] = layer['config']['name']\n",
    "#         layer['name'] = new_name\n",
    "        layer['config']['name'] = new_name\n",
    "\n",
    "    new_model = tf.keras.Sequential().from_config(config, custom_objects)\n",
    "    \n",
    "    for layer in new_model.layers:\n",
    "        layer.set_weights(model.get_layer(new_to_old[layer.name]).get_weights())\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(memebers_param):\n",
    "    all_models = list()\n",
    "    prefix = \"ensemble_\"\n",
    "    \n",
    "    for i, m in enumerate(memebers_param):\n",
    "        # define filename for this ensemble\n",
    "        filename = m[\"filePath\"]\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(add_prefix(model,prefix,str(i)))\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    " \n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(12, activation='relu')(merge)\n",
    "    output = Dense(N_LABELS, activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optmz, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = define_stacked_model(load_all_models(memebers_param))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1653206059750,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "oiFHNVtKaoMc",
    "outputId": "548eef54-ba28-4e79-f3d5-9e156fc04d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model: models/arbitrator_softmax_50_32_1_2022-10-08_00-05-20.hdf5\n",
      "Path to log:   models/arbitrator_softmax_50_32_1_2022-10-08_00-05-20.csv\n"
     ]
    }
   ],
   "source": [
    "                                                                                # Step 1\n",
    "modelname       = modelname+\"_\"+str(datetime.datetime.now())[:-7].replace(' ','_').replace(\":\",'-')\n",
    "folderpath      = 'models/'\n",
    "model_json      = folderpath + modelname + \".json\"\n",
    "with open(model_json, \"w\") as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "model_file      = folderpath + modelname + \".hdf5\"\n",
    "checkpoint      = ModelCheckpoint(model_file, \n",
    "                                  monitor='val_accuracy', \n",
    "                                  verbose=0, \n",
    "                                  save_best_only=True, \n",
    "                                  mode='max')\n",
    "\n",
    "csv_logger      = CSVLogger(folderpath+modelname +'.csv')                       # Step 2\n",
    "callbacks_list  = [checkpoint,csv_logger]                                       # Step 3\n",
    "\n",
    "print(\"Path to model:\", model_file)\n",
    "print(\"Path to log:  \", folderpath+modelname+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "104/104 [==============================] - 16s 70ms/step - loss: 3.1458 - accuracy: 0.2527 - val_loss: 3.1190 - val_accuracy: 0.3079\n",
      "Epoch 2/50\n",
      "104/104 [==============================] - 5s 45ms/step - loss: 3.1160 - accuracy: 0.4031 - val_loss: 3.0910 - val_accuracy: 0.4461\n",
      "Epoch 3/50\n",
      "104/104 [==============================] - 5s 45ms/step - loss: 3.0881 - accuracy: 0.4483 - val_loss: 3.0644 - val_accuracy: 0.5212\n",
      "Epoch 4/50\n",
      "104/104 [==============================] - 6s 52ms/step - loss: 3.0625 - accuracy: 0.5444 - val_loss: 3.0388 - val_accuracy: 0.5855\n",
      "Epoch 5/50\n",
      "104/104 [==============================] - 6s 48ms/step - loss: 3.0365 - accuracy: 0.5817 - val_loss: 3.0147 - val_accuracy: 0.6085\n",
      "Epoch 6/50\n",
      "104/104 [==============================] - 6s 46ms/step - loss: 3.0122 - accuracy: 0.5975 - val_loss: 2.9918 - val_accuracy: 0.6170\n",
      "Epoch 7/50\n",
      "104/104 [==============================] - 5s 45ms/step - loss: 2.9891 - accuracy: 0.6027 - val_loss: 2.9702 - val_accuracy: 0.6158\n",
      "Epoch 8/50\n",
      "104/104 [==============================] - 6s 46ms/step - loss: 2.9681 - accuracy: 0.6078 - val_loss: 2.9493 - val_accuracy: 0.6170\n",
      "Epoch 9/50\n",
      "104/104 [==============================] - 6s 46ms/step - loss: 2.9465 - accuracy: 0.6124 - val_loss: 2.9296 - val_accuracy: 0.6158\n",
      "Epoch 10/50\n",
      "104/104 [==============================] - 6s 48ms/step - loss: 2.9270 - accuracy: 0.6099 - val_loss: 2.9108 - val_accuracy: 0.6194\n",
      "Epoch 11/50\n",
      "104/104 [==============================] - 6s 47ms/step - loss: 2.9073 - accuracy: 0.6148 - val_loss: 2.8921 - val_accuracy: 0.6182\n",
      "Epoch 12/50\n",
      "104/104 [==============================] - 6s 48ms/step - loss: 2.8889 - accuracy: 0.6139 - val_loss: 2.8739 - val_accuracy: 0.6218\n",
      "Epoch 13/50\n",
      "104/104 [==============================] - 6s 48ms/step - loss: 2.8696 - accuracy: 0.6263 - val_loss: 2.8552 - val_accuracy: 0.6994\n",
      "Epoch 14/50\n",
      "104/104 [==============================] - 6s 48ms/step - loss: 2.8509 - accuracy: 0.7282 - val_loss: 2.8368 - val_accuracy: 0.7479\n",
      "Epoch 15/50\n",
      "104/104 [==============================] - 6s 47ms/step - loss: 2.8329 - accuracy: 0.7510 - val_loss: 2.8182 - val_accuracy: 0.7576\n",
      "Epoch 16/50\n",
      "104/104 [==============================] - 6s 50ms/step - loss: 2.8131 - accuracy: 0.7665 - val_loss: 2.8001 - val_accuracy: 0.7673\n",
      "Epoch 17/50\n",
      "104/104 [==============================] - 6s 49ms/step - loss: 2.7942 - accuracy: 0.7753 - val_loss: 2.7818 - val_accuracy: 0.7733\n",
      "Epoch 18/50\n",
      "104/104 [==============================] - 6s 49ms/step - loss: 2.7766 - accuracy: 0.7819 - val_loss: 2.7638 - val_accuracy: 0.7794\n",
      "Epoch 19/50\n",
      "104/104 [==============================] - 6s 48ms/step - loss: 2.7577 - accuracy: 0.7865 - val_loss: 2.7460 - val_accuracy: 0.7830\n",
      "Epoch 20/50\n",
      "104/104 [==============================] - 6s 48ms/step - loss: 2.7390 - accuracy: 0.7962 - val_loss: 2.7282 - val_accuracy: 0.7842\n",
      "Epoch 21/50\n",
      "104/104 [==============================] - 7s 54ms/step - loss: 2.7235 - accuracy: 0.7916 - val_loss: 2.7110 - val_accuracy: 0.7842\n",
      "Epoch 22/50\n",
      "104/104 [==============================] - 7s 56ms/step - loss: 2.7054 - accuracy: 0.7965 - val_loss: 2.6946 - val_accuracy: 0.7867\n",
      "Epoch 23/50\n",
      "104/104 [==============================] - 7s 53ms/step - loss: 2.6888 - accuracy: 0.7977 - val_loss: 2.6791 - val_accuracy: 0.7891\n",
      "Epoch 24/50\n",
      "104/104 [==============================] - 7s 54ms/step - loss: 2.6723 - accuracy: 0.8062 - val_loss: 2.6638 - val_accuracy: 0.7939\n",
      "Epoch 25/50\n",
      "104/104 [==============================] - 6s 52ms/step - loss: 2.6591 - accuracy: 0.8016 - val_loss: 2.6493 - val_accuracy: 0.7976\n",
      "Epoch 26/50\n",
      "104/104 [==============================] - 7s 54ms/step - loss: 2.6435 - accuracy: 0.8041 - val_loss: 2.6353 - val_accuracy: 0.7988\n",
      "Epoch 27/50\n",
      "104/104 [==============================] - 6s 52ms/step - loss: 2.6302 - accuracy: 0.8110 - val_loss: 2.6216 - val_accuracy: 0.7964\n",
      "Epoch 28/50\n",
      "104/104 [==============================] - 6s 51ms/step - loss: 2.6158 - accuracy: 0.8080 - val_loss: 2.6084 - val_accuracy: 0.7964\n",
      "Epoch 29/50\n",
      "104/104 [==============================] - 6s 51ms/step - loss: 2.6027 - accuracy: 0.8050 - val_loss: 2.5951 - val_accuracy: 0.7964\n",
      "Epoch 30/50\n",
      "104/104 [==============================] - 6s 49ms/step - loss: 2.5899 - accuracy: 0.8104 - val_loss: 2.5826 - val_accuracy: 0.7964\n",
      "Epoch 31/50\n",
      "104/104 [==============================] - 6s 49ms/step - loss: 2.5770 - accuracy: 0.8101 - val_loss: 2.5708 - val_accuracy: 0.7964\n",
      "Epoch 32/50\n",
      "104/104 [==============================] - 6s 48ms/step - loss: 2.5656 - accuracy: 0.8116 - val_loss: 2.5595 - val_accuracy: 0.7964\n",
      "Epoch 33/50\n",
      "104/104 [==============================] - 6s 51ms/step - loss: 2.5519 - accuracy: 0.8135 - val_loss: 2.5479 - val_accuracy: 0.7964\n",
      "Epoch 34/50\n",
      "104/104 [==============================] - 6s 52ms/step - loss: 2.5421 - accuracy: 0.8113 - val_loss: 2.5373 - val_accuracy: 0.7964\n",
      "Epoch 35/50\n",
      "104/104 [==============================] - 6s 51ms/step - loss: 2.5279 - accuracy: 0.8174 - val_loss: 2.5269 - val_accuracy: 0.7976\n",
      "Epoch 36/50\n",
      "104/104 [==============================] - 7s 54ms/step - loss: 2.5199 - accuracy: 0.8147 - val_loss: 2.5170 - val_accuracy: 0.7976\n",
      "Epoch 37/50\n",
      "104/104 [==============================] - 6s 52ms/step - loss: 2.5100 - accuracy: 0.8159 - val_loss: 2.5072 - val_accuracy: 0.7976\n",
      "Epoch 38/50\n",
      "104/104 [==============================] - 6s 53ms/step - loss: 2.5008 - accuracy: 0.8192 - val_loss: 2.4976 - val_accuracy: 0.7976\n",
      "Epoch 39/50\n",
      "104/104 [==============================] - 7s 53ms/step - loss: 2.4902 - accuracy: 0.8207 - val_loss: 2.4883 - val_accuracy: 0.7976\n",
      "Epoch 40/50\n",
      "104/104 [==============================] - 6s 52ms/step - loss: 2.4830 - accuracy: 0.8159 - val_loss: 2.4794 - val_accuracy: 0.7988\n",
      "Epoch 41/50\n",
      "104/104 [==============================] - 7s 53ms/step - loss: 2.4720 - accuracy: 0.8220 - val_loss: 2.4707 - val_accuracy: 0.7988\n",
      "Epoch 42/50\n",
      "104/104 [==============================] - 7s 54ms/step - loss: 2.4655 - accuracy: 0.8204 - val_loss: 2.4627 - val_accuracy: 0.8012\n",
      "Epoch 43/50\n",
      "104/104 [==============================] - 7s 53ms/step - loss: 2.4552 - accuracy: 0.8235 - val_loss: 2.4550 - val_accuracy: 0.8012\n",
      "Epoch 44/50\n",
      "104/104 [==============================] - 7s 55ms/step - loss: 2.4496 - accuracy: 0.8268 - val_loss: 2.4474 - val_accuracy: 0.8024\n",
      "Epoch 45/50\n",
      "104/104 [==============================] - 7s 54ms/step - loss: 2.4393 - accuracy: 0.8265 - val_loss: 2.4400 - val_accuracy: 0.8024\n",
      "Epoch 46/50\n",
      "104/104 [==============================] - 7s 57ms/step - loss: 2.4301 - accuracy: 0.8292 - val_loss: 2.4327 - val_accuracy: 0.8036\n",
      "Epoch 47/50\n",
      "104/104 [==============================] - 7s 54ms/step - loss: 2.4243 - accuracy: 0.8265 - val_loss: 2.4256 - val_accuracy: 0.8061\n",
      "Epoch 48/50\n",
      "104/104 [==============================] - 7s 54ms/step - loss: 2.4178 - accuracy: 0.8286 - val_loss: 2.4188 - val_accuracy: 0.8073\n",
      "Epoch 49/50\n",
      "104/104 [==============================] - 7s 55ms/step - loss: 2.4110 - accuracy: 0.8286 - val_loss: 2.4119 - val_accuracy: 0.8097\n",
      "Epoch 50/50\n",
      "104/104 [==============================] - 7s 54ms/step - loss: 2.4041 - accuracy: 0.8274 - val_loss: 2.4054 - val_accuracy: 0.8097\n",
      "duration =  5.34  minutes\n"
     ]
    }
   ],
   "source": [
    "import time as time\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(train_ds,                            # Training data and label\n",
    "          validation_data=val_ds,   # Validation data and label\n",
    "          epochs=EPOCHS,                       # The amount of epochs to be trained\n",
    "          batch_size=BATCH_SIZE,                   \n",
    "          shuffle=True,                     # To shuffle the training data\n",
    "          callbacks=callbacks_list)         # Callbacks to execute the checkpoints\n",
    "\n",
    "end = time.time()\n",
    "duration = round(((end - start)/60), 2)\n",
    "print(\"duration = \", duration, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "YSN4w8WeaoMd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to plot: models/arbitrator_softmax_50_32_1_2022-10-08_00-05-20_plot.png\n"
     ]
    }
   ],
   "source": [
    "plotpath  = folderpath+modelname+'_plot.png'\n",
    "plot_model(model, \n",
    "           to_file=plotpath, \n",
    "           show_shapes=True, \n",
    "           show_layer_names=False,\n",
    "           rankdir='TB')\n",
    "print(\"Path to plot:\", plotpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 3s 37ms/step\n",
      "Prediction completes.\n"
     ]
    }
   ],
   "source": [
    "                                                                                # Step 1\n",
    "modelGo = load_model(model_file)\n",
    "\n",
    "predicts    = modelGo.predict(val_ds)                                            # Step 2\n",
    "print(\"Prediction completes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy (on testing dataset): 80.97%\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     arowana     0.7617    0.9591    0.8491       220\n",
      "       betta     0.9610    0.7831    0.8630       189\n",
      "    goldfish     0.7346    0.5336    0.6182       223\n",
      "      luohan     0.8190    0.9845    0.8941       193\n",
      "\n",
      "    accuracy                         0.8097       825\n",
      "   macro avg     0.8191    0.8151    0.8061       825\n",
      "weighted avg     0.8134    0.8097    0.8004       825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "                                                                                # Step 1\n",
    "                                                                                # Step 2\n",
    "predout     = np.argmax(predicts,axis=1)\n",
    "testout     = np.argmax(y_val_bin,axis=1)\n",
    "\n",
    "testScores  = metrics.accuracy_score(testout,predout)                           # Step 3\n",
    "\n",
    "                                                                                # Step 4\n",
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100))\n",
    "print(metrics.classification_report(testout,\n",
    "                                    predout,\n",
    "                                    target_names=class_names,\n",
    "                                    digits=4))\n",
    "\n",
    "report = metrics.classification_report(testout,\n",
    "                                    predout,\n",
    "                                    target_names=class_names,\n",
    "                                    digits=4,\n",
    "                                      output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(folderpath+modelname+'_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "D5 StackingEnsembleNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
