{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "STVzL9mAaoMa"
   },
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions of key libraries\n",
      "---\n",
      "tensorflow:  2.9.0\n",
      "numpy:       1.22.3\n",
      "matplotlib:  3.5.2\n",
      "sklearn:     1.1.1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Versions of key libraries\")\n",
    "print(\"---\")\n",
    "print(\"tensorflow: \", tf.__version__)\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25812,
     "status": "ok",
     "timestamp": 1653205963445,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "6NqhUlrAaoMb",
    "outputId": "a3b1e3e9-d797-426a-dee9-136eb6d623f7"
   },
   "outputs": [],
   "source": [
    "members_param = [\n",
    "    {\n",
    "        \"model_name\": \"general\",\n",
    "        \"file_path\": \"./models/medium_best.hdf5\",\n",
    "        \"class_names\": ['arowana', 'betta', 'goldfish', 'flowerhorn'],\n",
    "        \"img_width\": 224,\n",
    "        \"img_height\":160\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"arowana\",\n",
    "        \"file_path\": \"./models/arowana_best.hdf5\",\n",
    "        \"class_names\": ['arowana', 'not arowana'],\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"betta\",\n",
    "        \"file_path\": \"./models/betta_best.hdf5\",\n",
    "        \"class_names\": ['betta', 'not betta'],\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"goldfish\",\n",
    "        \"file_path\": \"./models/goldfish_best.hdf5\",\n",
    "        \"class_names\": ['goldfish', 'not goldfish'],\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"model_name\": \"flowerhorn\",\n",
    "        \"file_path\": \"./models/luohan_best.hdf5\",\n",
    "        \"class_names\": ['flowerhorn', 'not flowerhorn'],\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    }\n",
    "]\n",
    "\n",
    "modelname   = 'arbitrator'\n",
    "BATCH_SIZE = 32 # Big enough to measure an F1-score\n",
    "AUTOTUNE = tf.data.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 160\n",
    "CHANNELS = 3\n",
    "SEED = 7\n",
    "N_LABELS = 4\n",
    "EPOCHS = 50\n",
    "OPT_IDX = 1\n",
    "ACTIVATION = 'softmax'\n",
    "\n",
    "class_names = ''\n",
    "modelname = modelname+\"_\"+ACTIVATION+\"_\"+str(EPOCHS)+\"_\"+str(BATCH_SIZE)+\"_\"+str(OPT_IDX)\n",
    "optmzs = ['adam', optimizers.RMSprop(learning_rate=0.0001)]\n",
    "optmz = optmzs[OPT_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2873,
     "status": "ok",
     "timestamp": 1653205981198,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "ODxtvchaaoMc",
    "outputId": "7a09613d-babb-43c8-cee7-065c71792ce0"
   },
   "outputs": [],
   "source": [
    "def readImagesFromDir(base_img_path='dataset/'):\n",
    "    dirs = [d for d in listdir(base_img_path) if isdir(join(base_img_path, d)) and not d.startswith('.') and not d in ['oranda', 'common_goldfish']]\n",
    "\n",
    "    print(dirs)\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for d in dirs:\n",
    "        img_path = base_img_path + d + \"/\"\n",
    "        files = [f for f in listdir(img_path) if isfile(join(img_path, f))]\n",
    "        X = X + [os.path.join(img_path, f) for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        y = y + [d for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        #print(d)\n",
    "\n",
    "    data_dir = Path(base_img_path)\n",
    "    image_count = len(list(data_dir.glob('*/*.*')))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def preprocess_image(filename, label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    # Read an image from a file\n",
    "    images = {}\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "#     image_resized = tf.image.resize_with_pad(image_decoded, IMG_HEIGHT, IMG_WIDTH, antialias=False)\n",
    "#     image_normalized = image_resized / 255.0\n",
    "    for i,m in enumerate(members_param):\n",
    "        image_resized = tf.image.resize_with_pad(image_decoded, m[\"img_height\"], m[\"img_width\"], antialias=False)\n",
    "        image_normalized = image_resized / 255.0\n",
    "        images[\"input_\"+str(i)] = image_normalized\n",
    "        \n",
    "    return images, label\n",
    "\n",
    "\n",
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    \n",
    "#     imageDS = \n",
    "    \n",
    "#     labelDS = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if is_training == True:\n",
    "        #dataset = dataset.take(BATCH_SIZE)\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        #dataset = dataset.cache()\n",
    "        #dataset = dataset.repeat()\n",
    "        # Shuffle the data each buffer size\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "        #dataset = dataset.repeat()\n",
    "        \n",
    "    # Batch the data for multiple steps    \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def visualize_image(original, augmented):\n",
    "    org_img = tf.keras.utils.array_to_img(original)\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img)\n",
    "\n",
    "    aug_img = tf.keras.utils.array_to_img(augmented)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Augmented image')\n",
    "    plt.imshow(aug_img)\n",
    "\n",
    "\n",
    "def predict_new_image(img_file):    \n",
    "    #img = tf.keras.utils.load_img(\n",
    "    #    img_file, target_size=(IMG_HEIGHT, IMG_WIDTH), keep_aspect_ratio=True\n",
    "    #)\n",
    "    img = tf.keras.utils.load_img(\n",
    "        img_file, target_size=None, keep_aspect_ratio=True\n",
    "    )\n",
    "\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.keras.preprocessing.image.smart_resize(img_array, size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    image_normalized = img_array / 255.0  # tf.image.per_image_standardization(img_array)\n",
    "\n",
    "    saved_model = load_model(model_file)\n",
    "\n",
    "    predictions = saved_model.predict(image_normalized)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "    #score = tf.sigmoid(predictions[0])\n",
    "    #score = tf.nn.sigmoid_cross_entropy_with_logits(labels=class_names, logits=predictions[0])\n",
    "    #score = tf.math.sigmoid(predictions[0])\n",
    "    #score = tf.tanh(predictions[0])\n",
    "\n",
    "    #model.evaluate(img_array)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    #print(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arowana', 'betta', 'goldfish', 'luohan']\n",
      "0. arowana\n",
      "1. betta\n",
      "2. goldfish\n",
      "3. luohan\n",
      "(9166, 4)\n",
      "(2292, 4)\n"
     ]
    }
   ],
   "source": [
    "X, Y = readImagesFromDir()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=SEED)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "class_names = lb.classes_\n",
    "# Loop over all labels and show them    \n",
    "N_LABELS = len(class_names)\n",
    "for (i, label) in enumerate(class_names):\n",
    "    print(\"{}. {}\".format(i, label))\n",
    "\n",
    "# transform the targets of the training and test sets\n",
    "y_train_bin = lb.transform(y_train)\n",
    "y_val_bin = lb.transform(y_val)\n",
    "\n",
    "print(y_train_bin.shape)\n",
    "print(y_val_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/luohan/Flowerhorn10380.jpg [0 0 0 1]\n",
      "dataset/arowana/240_F_73601716_vduxsAMnoi1s681yzgQQa7WmvEUNeFuQ.jpg [1 0 0 0]\n",
      "dataset/luohan/Flowerhorn11710.jpg [0 0 0 1]\n",
      "dataset/betta/BettaFish11692.jpg [0 1 0 0]\n",
      "dataset/betta/BettaFish10300.jpg [0 1 0 0]\n",
      "dataset/arowana/Arowana_100303.jpg [1 0 0 0]\n",
      "dataset/arowana/Arowana_100756.jpg [1 0 0 0]\n",
      "dataset/luohan/Red-Texas-Flowerhorn.jpg [0 0 0 1]\n",
      "dataset/goldfish/Goldfish2517.jpg [0 0 1 0]\n",
      "dataset/arowana/Arowana_101967.jpg [1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Print examples of fish and their binary targets\n",
    "for i in range(10):\n",
    "    print(X_train[len(X_train)-1 - i], y_train_bin[len(y_train_bin)-1 - i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = []\n",
    "# val_ds = []\n",
    "\n",
    "train_ds = create_dataset(X_train, y_train_bin)\n",
    "val_ds = create_dataset(X_val, y_val_bin, is_training=False)\n",
    "\n",
    "# for images, labels in train_ds.take(1):\n",
    "#     print(\"Shape of features array:\", images.numpy().shape)\n",
    "#     print(\"Shape of labels array:\", labels.numpy().shape)\n",
    "#     #plt.imshow(f.numpy().astype(\"uint8\"))\n",
    "#     for i in range(5):\n",
    "#         ax = plt.subplot(2, 3, i + 1)\n",
    "#         img = tf.keras.utils.array_to_img(images[i])\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(class_names[np.argmax(labels[i])])\n",
    "#         plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8122,
     "status": "ok",
     "timestamp": 1653206045624,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "gRdnqYDSaoMc",
    "outputId": "417b2730-29c8-4554-b302-5715ef394613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded ./models/medium_best.hdf5\n",
      ">loaded ./models/arowana_best.hdf5\n",
      ">loaded ./models/betta_best.hdf5\n",
      ">loaded ./models/goldfish_best.hdf5\n",
      ">loaded ./models/luohan_best.hdf5\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_0 (InputLayer)           [(None, 160, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_12 (Conv2D)   (None, 160, 224, 32  2432        ['input_0[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_12 (Ma  (None, 80, 112, 32)  0          ['ensemble_0conv2d_12[0][0]']    \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_13 (Conv2D)   (None, 80, 112, 64)  51264       ['ensemble_0max_pooling2d_12[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_13 (Ma  (None, 40, 56, 64)  0           ['ensemble_0conv2d_13[0][0]']    \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_14 (Conv2D)   (None, 40, 56, 128)  204928      ['ensemble_0max_pooling2d_13[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_14 (Ma  (None, 20, 28, 128)  0          ['ensemble_0conv2d_14[0][0]']    \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_15 (Conv2D)   (None, 20, 28, 256)  524544      ['ensemble_0max_pooling2d_14[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 128, 128, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_15 (Ma  (None, 10, 14, 256)  0          ['ensemble_0conv2d_15[0][0]']    \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " ensemble_1sequential (Sequenti  (None, 128, 128, 3)  0          ['input_1[0][0]']                \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " ensemble_2sequential (Sequenti  (None, 128, 128, 3)  0          ['input_2[0][0]']                \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " ensemble_3sequential (Sequenti  (None, 128, 128, 3)  0          ['input_3[0][0]']                \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " ensemble_4sequential (Sequenti  (None, 128, 128, 3)  0          ['input_4[0][0]']                \n",
      " al)                                                                                              \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_16 (Conv2D)   (None, 10, 14, 512)  2097664     ['ensemble_0max_pooling2d_15[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " ensemble_1conv2d (Conv2D)      (None, 128, 128, 32  896         ['ensemble_1sequential[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_2conv2d (Conv2D)      (None, 128, 128, 32  896         ['ensemble_2sequential[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_3conv2d (Conv2D)      (None, 128, 128, 32  896         ['ensemble_3sequential[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_4conv2d (Conv2D)      (None, 128, 128, 32  896         ['ensemble_4sequential[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_16 (Ma  (None, 5, 7, 512)   0           ['ensemble_0conv2d_16[0][0]']    \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " ensemble_1max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_1conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_2max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_2conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_3max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_3conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ensemble_4max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_4conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_17 (Conv2D)   (None, 5, 7, 768)    3539712     ['ensemble_0max_pooling2d_16[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " ensemble_1dropout (Dropout)    (None, 64, 64, 32)   0           ['ensemble_1max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_2dropout (Dropout)    (None, 64, 64, 32)   0           ['ensemble_2max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_3dropout (Dropout)    (None, 64, 64, 32)   0           ['ensemble_3max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_4dropout (Dropout)    (None, 64, 64, 32)   0           ['ensemble_4max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_17 (Ma  (None, 2, 3, 768)   0           ['ensemble_0conv2d_17[0][0]']    \n",
      " xPooling2D)                                                                                      \n",
      "                                                                                                  \n",
      " ensemble_1conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_1dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_2conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_2dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_3conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_3dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_4conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_4dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_0dropout_2 (Dropout)  (None, 2, 3, 768)    0           ['ensemble_0max_pooling2d_17[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " ensemble_1max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_1conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_2max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_2conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_3max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_3conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_4max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_4conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_0flatten_2 (Flatten)  (None, 4608)         0           ['ensemble_0dropout_2[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1dropout_1 (Dropout)  (None, 32, 32, 16)   0           ['ensemble_1max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_2dropout_1 (Dropout)  (None, 32, 32, 16)   0           ['ensemble_2max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_3dropout_1 (Dropout)  (None, 32, 32, 16)   0           ['ensemble_3max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_4dropout_1 (Dropout)  (None, 32, 32, 16)   0           ['ensemble_4max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_0dense_14 (Dense)     (None, 768)          3539712     ['ensemble_0flatten_2[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1conv2d_2 (Conv2D)    (None, 32, 32, 8)    1160        ['ensemble_1dropout_1[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_2conv2d_2 (Conv2D)    (None, 32, 32, 8)    1160        ['ensemble_2dropout_1[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_3conv2d_2 (Conv2D)    (None, 32, 32, 8)    1160        ['ensemble_3dropout_1[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_4conv2d_2 (Conv2D)    (None, 32, 32, 8)    1160        ['ensemble_4dropout_1[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_0dense_15 (Dense)     (None, 512)          393728      ['ensemble_0dense_14[0][0]']     \n",
      "                                                                                                  \n",
      " ensemble_1max_pooling2d_2 (Max  (None, 16, 16, 8)   0           ['ensemble_1conv2d_2[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_2max_pooling2d_2 (Max  (None, 16, 16, 8)   0           ['ensemble_2conv2d_2[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_3max_pooling2d_2 (Max  (None, 16, 16, 8)   0           ['ensemble_3conv2d_2[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_4max_pooling2d_2 (Max  (None, 16, 16, 8)   0           ['ensemble_4conv2d_2[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_0dense_16 (Dense)     (None, 256)          131328      ['ensemble_0dense_15[0][0]']     \n",
      "                                                                                                  \n",
      " ensemble_1dropout_2 (Dropout)  (None, 16, 16, 8)    0           ['ensemble_1max_pooling2d_2[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ensemble_2dropout_2 (Dropout)  (None, 16, 16, 8)    0           ['ensemble_2max_pooling2d_2[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_3dropout_2 (Dropout)  (None, 16, 16, 8)    0           ['ensemble_3max_pooling2d_2[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_4dropout_2 (Dropout)  (None, 16, 16, 8)    0           ['ensemble_4max_pooling2d_2[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_0dense_17 (Dense)     (None, 64)           16448       ['ensemble_0dense_16[0][0]']     \n",
      "                                                                                                  \n",
      " ensemble_1flatten (Flatten)    (None, 2048)         0           ['ensemble_1dropout_2[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_2flatten (Flatten)    (None, 2048)         0           ['ensemble_2dropout_2[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_3flatten (Flatten)    (None, 2048)         0           ['ensemble_3dropout_2[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_4flatten (Flatten)    (None, 2048)         0           ['ensemble_4dropout_2[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_0dense_18 (Dense)     (None, 32)           2080        ['ensemble_0dense_17[0][0]']     \n",
      "                                                                                                  \n",
      " ensemble_1dense (Dense)        (None, 64)           131136      ['ensemble_1flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_2dense (Dense)        (None, 64)           131136      ['ensemble_2flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_3dense (Dense)        (None, 64)           131136      ['ensemble_3flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_4dense (Dense)        (None, 64)           131136      ['ensemble_4flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_0dense_19 (Dense)     (None, 16)           528         ['ensemble_0dense_18[0][0]']     \n",
      "                                                                                                  \n",
      " ensemble_1dense_1 (Dense)      (None, 16)           1040        ['ensemble_1dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_2dense_1 (Dense)      (None, 16)           1040        ['ensemble_2dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_3dense_1 (Dense)      (None, 16)           1040        ['ensemble_3dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_4dense_1 (Dense)      (None, 16)           1040        ['ensemble_4dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_0dense_20 (Dense)     (None, 4)            68          ['ensemble_0dense_19[0][0]']     \n",
      "                                                                                                  \n",
      " ensemble_1dense_2 (Dense)      (None, 2)            34          ['ensemble_1dense_1[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_2dense_2 (Dense)      (None, 2)            34          ['ensemble_2dense_1[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_3dense_2 (Dense)      (None, 2)            34          ['ensemble_3dense_1[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_4dense_2 (Dense)      (None, 2)            34          ['ensemble_4dense_1[0][0]']      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 12)           0           ['ensemble_0dense_20[0][0]',     \n",
      "                                                                  'ensemble_1dense_2[0][0]',      \n",
      "                                                                  'ensemble_2dense_2[0][0]',      \n",
      "                                                                  'ensemble_3dense_2[0][0]',      \n",
      "                                                                  'ensemble_4dense_2[0][0]']      \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 12)           156         ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 4)            52          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,060,204\n",
      "Trainable params: 208\n",
      "Non-trainable params: 11,059,996\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def add_prefix(model, prefix: str, i:str, custom_objects=None):\n",
    "    config = model.get_config()\n",
    "    new_to_old = {}\n",
    "    for layer in config['layers']:\n",
    "#         print(layer)\n",
    "        new_name = prefix + i + layer['config']['name']\n",
    "        if layer['class_name']=='InputLayer':\n",
    "            new_name = \"input_\"+i\n",
    "        new_to_old[new_name] = layer['config']['name']\n",
    "#         layer['name'] = new_name\n",
    "        layer['config']['name'] = new_name\n",
    "    if config['name'].startswith('sequential'):\n",
    "        new_model = tf.keras.Sequential().from_config(config, custom_objects)\n",
    "    else:\n",
    "        new_model = tf.keras.Model().from_config(config, custom_objects)\n",
    "    for layer in new_model.layers:\n",
    "        layer.set_weights(model.get_layer(new_to_old[layer.name]).get_weights())\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(members_param):\n",
    "    all_models = list()\n",
    "    prefix = \"ensemble_\"\n",
    "    \n",
    "    for i, m in enumerate(members_param):\n",
    "        # define filename for this ensemble\n",
    "        filename = m[\"file_path\"]\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(add_prefix(model,prefix,str(i)))\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    " \n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(12, activation='relu')(merge)\n",
    "    output = Dense(N_LABELS, activation=ACTIVATION)(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optmz, metrics=['accuracy'])\n",
    "    return model\n",
    "member_models = load_all_models(members_param)\n",
    "model = define_stacked_model(member_models)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1653206059750,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "oiFHNVtKaoMc",
    "outputId": "548eef54-ba28-4e79-f3d5-9e156fc04d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model: models/arbitrator_softmax_50_32_1_2022-10-17_00-21-39.hdf5\n",
      "Path to log:   models/arbitrator_softmax_50_32_1_2022-10-17_00-21-39.csv\n"
     ]
    }
   ],
   "source": [
    "                                                                                # Step 1\n",
    "modelname       = modelname+\"_\"+str(datetime.datetime.now())[:-7].replace(' ','_').replace(\":\",'-')\n",
    "folderpath      = 'models/'\n",
    "model_json      = folderpath + modelname + \".json\"\n",
    "with open(model_json, \"w\") as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "model_file      = folderpath + modelname + \".hdf5\"\n",
    "checkpoint      = ModelCheckpoint(model_file, \n",
    "                                  monitor='val_accuracy', \n",
    "                                  verbose=0, \n",
    "                                  save_best_only=True, \n",
    "                                  mode='max')\n",
    "\n",
    "csv_logger      = CSVLogger(folderpath+modelname +'.csv')                       # Step 2\n",
    "callbacks_list  = [checkpoint,csv_logger]                                       # Step 3\n",
    "\n",
    "print(\"Path to model:\", model_file)\n",
    "print(\"Path to log:  \", folderpath+modelname+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "287/287 [==============================] - 57s 165ms/step - loss: 2.0807 - accuracy: 0.3299 - val_loss: 2.0100 - val_accuracy: 0.4660\n",
      "Epoch 2/50\n",
      "287/287 [==============================] - 47s 160ms/step - loss: 1.9515 - accuracy: 0.5112 - val_loss: 1.8898 - val_accuracy: 0.6078\n",
      "Epoch 3/50\n",
      "287/287 [==============================] - 47s 157ms/step - loss: 1.8300 - accuracy: 0.6717 - val_loss: 1.7744 - val_accuracy: 0.6745\n",
      "Epoch 4/50\n",
      "287/287 [==============================] - 47s 160ms/step - loss: 1.7127 - accuracy: 0.6968 - val_loss: 1.6589 - val_accuracy: 0.6950\n",
      "Epoch 5/50\n",
      "287/287 [==============================] - 49s 166ms/step - loss: 1.6019 - accuracy: 0.7087 - val_loss: 1.5515 - val_accuracy: 0.7308\n",
      "Epoch 6/50\n",
      "287/287 [==============================] - 46s 158ms/step - loss: 1.5003 - accuracy: 0.7599 - val_loss: 1.4539 - val_accuracy: 0.8421\n",
      "Epoch 7/50\n",
      "287/287 [==============================] - 49s 168ms/step - loss: 1.4099 - accuracy: 0.8599 - val_loss: 1.3645 - val_accuracy: 0.8774\n",
      "Epoch 8/50\n",
      "287/287 [==============================] - 50s 167ms/step - loss: 1.3277 - accuracy: 0.8896 - val_loss: 1.2857 - val_accuracy: 0.8905\n",
      "Epoch 9/50\n",
      "287/287 [==============================] - 47s 160ms/step - loss: 1.2553 - accuracy: 0.9014 - val_loss: 1.2153 - val_accuracy: 0.9023\n",
      "Epoch 10/50\n",
      "287/287 [==============================] - 49s 166ms/step - loss: 1.1911 - accuracy: 0.9134 - val_loss: 1.1550 - val_accuracy: 0.9097\n",
      "Epoch 11/50\n",
      "287/287 [==============================] - 48s 162ms/step - loss: 1.1324 - accuracy: 0.9172 - val_loss: 1.1011 - val_accuracy: 0.9162\n",
      "Epoch 12/50\n",
      "287/287 [==============================] - 48s 163ms/step - loss: 1.0796 - accuracy: 0.9202 - val_loss: 1.0535 - val_accuracy: 0.9228\n",
      "Epoch 13/50\n",
      "287/287 [==============================] - 47s 159ms/step - loss: 1.0369 - accuracy: 0.9245 - val_loss: 1.0130 - val_accuracy: 0.9263\n",
      "Epoch 14/50\n",
      "287/287 [==============================] - 47s 159ms/step - loss: 0.9998 - accuracy: 0.9273 - val_loss: 0.9787 - val_accuracy: 0.9289\n",
      "Epoch 15/50\n",
      "287/287 [==============================] - 49s 165ms/step - loss: 0.9684 - accuracy: 0.9294 - val_loss: 0.9524 - val_accuracy: 0.9298\n",
      "Epoch 16/50\n",
      "287/287 [==============================] - 49s 166ms/step - loss: 0.9472 - accuracy: 0.9291 - val_loss: 0.9317 - val_accuracy: 0.9311\n",
      "Epoch 17/50\n",
      "287/287 [==============================] - 48s 164ms/step - loss: 0.9296 - accuracy: 0.9288 - val_loss: 0.9161 - val_accuracy: 0.9319\n",
      "Epoch 18/50\n",
      "287/287 [==============================] - 47s 161ms/step - loss: 0.9179 - accuracy: 0.9291 - val_loss: 0.9049 - val_accuracy: 0.9324\n",
      "Epoch 19/50\n",
      "287/287 [==============================] - 47s 159ms/step - loss: 0.9062 - accuracy: 0.9307 - val_loss: 0.8966 - val_accuracy: 0.9324\n",
      "Epoch 20/50\n",
      "287/287 [==============================] - 47s 158ms/step - loss: 0.9010 - accuracy: 0.9292 - val_loss: 0.8898 - val_accuracy: 0.9350\n",
      "Epoch 21/50\n",
      "287/287 [==============================] - 46s 157ms/step - loss: 0.8933 - accuracy: 0.9316 - val_loss: 0.8846 - val_accuracy: 0.9350\n",
      "Epoch 22/50\n",
      "287/287 [==============================] - 48s 163ms/step - loss: 0.8919 - accuracy: 0.9315 - val_loss: 0.8807 - val_accuracy: 0.9359\n",
      "Epoch 23/50\n",
      "287/287 [==============================] - 47s 159ms/step - loss: 0.8878 - accuracy: 0.9318 - val_loss: 0.8777 - val_accuracy: 0.9367\n",
      "Epoch 24/50\n",
      "287/287 [==============================] - 47s 160ms/step - loss: 0.8857 - accuracy: 0.9312 - val_loss: 0.8756 - val_accuracy: 0.9354\n",
      "Epoch 25/50\n",
      "287/287 [==============================] - 54s 185ms/step - loss: 0.8853 - accuracy: 0.9316 - val_loss: 0.8743 - val_accuracy: 0.9346\n",
      "Epoch 26/50\n",
      "287/287 [==============================] - 49s 161ms/step - loss: 0.8836 - accuracy: 0.9318 - val_loss: 0.8728 - val_accuracy: 0.9350\n",
      "Epoch 27/50\n",
      "287/287 [==============================] - 47s 159ms/step - loss: 0.8812 - accuracy: 0.9305 - val_loss: 0.8713 - val_accuracy: 0.9346\n",
      "Epoch 28/50\n",
      "287/287 [==============================] - 47s 161ms/step - loss: 0.8807 - accuracy: 0.9315 - val_loss: 0.8705 - val_accuracy: 0.9346\n",
      "Epoch 29/50\n",
      "287/287 [==============================] - 47s 160ms/step - loss: 0.8833 - accuracy: 0.9297 - val_loss: 0.8696 - val_accuracy: 0.9346\n",
      "Epoch 30/50\n",
      "287/287 [==============================] - 48s 161ms/step - loss: 0.8803 - accuracy: 0.9314 - val_loss: 0.8687 - val_accuracy: 0.9354\n",
      "Epoch 31/50\n",
      "287/287 [==============================] - 50s 169ms/step - loss: 0.8792 - accuracy: 0.9318 - val_loss: 0.8683 - val_accuracy: 0.9346\n",
      "Epoch 32/50\n",
      "287/287 [==============================] - 49s 166ms/step - loss: 0.8768 - accuracy: 0.9325 - val_loss: 0.8678 - val_accuracy: 0.9350\n",
      "Epoch 33/50\n",
      "287/287 [==============================] - 50s 171ms/step - loss: 0.8795 - accuracy: 0.9309 - val_loss: 0.8676 - val_accuracy: 0.9350\n",
      "Epoch 34/50\n",
      "287/287 [==============================] - 51s 173ms/step - loss: 0.8797 - accuracy: 0.9326 - val_loss: 0.8671 - val_accuracy: 0.9354\n",
      "Epoch 35/50\n",
      "287/287 [==============================] - 50s 170ms/step - loss: 0.8791 - accuracy: 0.9314 - val_loss: 0.8670 - val_accuracy: 0.9350\n",
      "Epoch 36/50\n",
      "287/287 [==============================] - 51s 173ms/step - loss: 0.8752 - accuracy: 0.9327 - val_loss: 0.8666 - val_accuracy: 0.9354\n",
      "Epoch 37/50\n",
      " 58/287 [=====>........................] - ETA: 42s - loss: 0.9077 - accuracy: 0.9230"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Graph execution error:\n\n2 root error(s) found.\n  (0) NOT_FOUND:  NewRandomAccessFile failed to Create/Open: dataset/luohan/227_1500355650.jpg : The system cannot find the file specified.\r\n; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_10]]\n  (1) NOT_FOUND:  NewRandomAccessFile failed to Create/Open: dataset/luohan/227_1500355650.jpg : The system cannot find the file specified.\r\n; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_9353]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# Training data and label\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Validation data and label\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# The amount of epochs to be trained\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# To shuffle the training data\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# Callbacks to execute the checkpoints\u001b[39;00m\n\u001b[0;32m     11\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(((end \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mNotFoundError\u001b[0m: Graph execution error:\n\n2 root error(s) found.\n  (0) NOT_FOUND:  NewRandomAccessFile failed to Create/Open: dataset/luohan/227_1500355650.jpg : The system cannot find the file specified.\r\n; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_10]]\n  (1) NOT_FOUND:  NewRandomAccessFile failed to Create/Open: dataset/luohan/227_1500355650.jpg : The system cannot find the file specified.\r\n; No such file or directory\n\t [[{{node ReadFile}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_9353]"
     ]
    }
   ],
   "source": [
    "import time as time\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(train_ds,                            # Training data and label\n",
    "          validation_data=val_ds,   # Validation data and label\n",
    "          epochs=EPOCHS,                       # The amount of epochs to be trained\n",
    "          batch_size=BATCH_SIZE,                   \n",
    "          shuffle=True,                     # To shuffle the training data\n",
    "          callbacks=callbacks_list)         # Callbacks to execute the checkpoints\n",
    "\n",
    "end = time.time()\n",
    "duration = round(((end - start)/60), 2)\n",
    "print(\"duration = \", duration, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSN4w8WeaoMd"
   },
   "outputs": [],
   "source": [
    "plotpath  = folderpath+modelname+'_plot.png'\n",
    "plot_model(model, \n",
    "           to_file=plotpath, \n",
    "           show_shapes=True, \n",
    "           show_layer_names=False,\n",
    "           rankdir='TB')\n",
    "print(\"Path to plot:\", plotpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                # Step 1\n",
    "modelGo = load_model(model_file)\n",
    "\n",
    "predicts    = modelGo.predict(val_ds)                                            # Step 2\n",
    "print(\"Prediction completes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                                                                # Step 1\n",
    "                                                                                # Step 2\n",
    "predout     = np.argmax(predicts,axis=1)\n",
    "testout     = np.argmax(y_val_bin,axis=1)\n",
    "\n",
    "testScores  = metrics.accuracy_score(testout,predout)                           # Step 3\n",
    "\n",
    "                                                                                # Step 4\n",
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100))\n",
    "print(metrics.classification_report(testout,\n",
    "                                    predout,\n",
    "                                    target_names=class_names,\n",
    "                                    digits=4))\n",
    "\n",
    "report = metrics.classification_report(testout,\n",
    "                                    predout,\n",
    "                                    target_names=class_names,\n",
    "                                    digits=4,\n",
    "                                      output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(folderpath+modelname+'_report.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resdf = pd.DataFrame()\n",
    "resdf[\"file\"] = X_val\n",
    "resdf[\"fish\"] = y_val\n",
    "resdf[\"testout\"] = testout\n",
    "resdf[\"predout\"] = predout\n",
    "print(resdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print all wrong predition\n",
    "resdf[resdf.testout !=resdf.predout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "D5 StackingEnsembleNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
