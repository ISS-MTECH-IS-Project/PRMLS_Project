{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "STVzL9mAaoMa"
   },
   "outputs": [],
   "source": [
    "# pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versions of key libraries\n",
      "---\n",
      "tensorflow:  2.9.0\n",
      "numpy:       1.22.3\n",
      "matplotlib:  3.5.2\n",
      "sklearn:     1.1.1\n",
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import cv2\n",
    "import datetime\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, isdir, join\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,CSVLogger\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import regularizers\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Versions of key libraries\")\n",
    "print(\"---\")\n",
    "print(\"tensorflow: \", tf.__version__)\n",
    "print(\"numpy:      \", np.__version__)\n",
    "print(\"matplotlib: \", matplotlib.__version__)\n",
    "print(\"sklearn:    \", sklearn.__version__)\n",
    "print(tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25812,
     "status": "ok",
     "timestamp": 1653205963445,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "6NqhUlrAaoMb",
    "outputId": "a3b1e3e9-d797-426a-dee9-136eb6d623f7"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "memebers_param = [\n",
    "    {\n",
    "        \"fishType\": \"general\",\n",
    "        \"filePath\": \"./models/medium_224_20_64_1_2022-09-25_19-33-18.hdf5\",\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"fishType\": \"arawana\",\n",
    "        \"filePath\": \"./models/arowana_softmax_128_50_64_1_2022-09-25_22-34-38.hdf5\",\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"fishType\": \"betta\",\n",
    "        \"filePath\": \"./models/betta_softmax_128_50_64_1_2022-09-26_17-51-39.hdf5\",\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"fishType\": \"goldfish\",\n",
    "        \"filePath\": \"./models/goldfish_softmax_128_50_64_1_2022-09-25_22-37-40.hdf5\",\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    },\n",
    "    {\n",
    "        \"fishType\": \"flowerhorn\",\n",
    "        \"filePath\": \"./models/luohan_softmax_128_50_64_1_2022-09-25_22-42-17.hdf5\",\n",
    "        \"img_width\": 128,\n",
    "        \"img_height\":128\n",
    "    }\n",
    "]\n",
    "\n",
    "modelname   = 'arbitrator'\n",
    "BATCH_SIZE = 32 # Big enough to measure an F1-score\n",
    "AUTOTUNE = tf.data.AUTOTUNE # Adapt preprocessing and prefetching dynamically\n",
    "SHUFFLE_BUFFER_SIZE = 1024 # Shuffle the training data by a chunck of 1024 observations\n",
    "IMG_WIDTH = 224\n",
    "IMG_HEIGHT = 160\n",
    "CHANNELS = 3\n",
    "SEED = 7\n",
    "N_LABELS = 4\n",
    "EPOCHS = 50\n",
    "OPT_IDX = 1\n",
    "ACTIVATION = 'softmax'\n",
    "\n",
    "class_names = ''\n",
    "modelname = modelname+\"_\"+ACTIVATION+\"_\"+str(EPOCHS)+\"_\"+str(BATCH_SIZE)+\"_\"+str(OPT_IDX)\n",
    "optmzs = ['adam', optimizers.RMSprop(learning_rate=0.0001)]\n",
    "optmz = optmzs[OPT_IDX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2873,
     "status": "ok",
     "timestamp": 1653205981198,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "ODxtvchaaoMc",
    "outputId": "7a09613d-babb-43c8-cee7-065c71792ce0"
   },
   "outputs": [],
   "source": [
    "def readImagesFromDir(base_img_path='dataset/'):\n",
    "    dirs = [d for d in listdir(base_img_path) if isdir(join(base_img_path, d)) and not d.startswith('.') and not d in ['oranda', 'common_goldfish']]\n",
    "\n",
    "    print(dirs)\n",
    "\n",
    "    X, y = [], []\n",
    "\n",
    "    for d in dirs:\n",
    "        img_path = base_img_path + d + \"/\"\n",
    "        files = [f for f in listdir(img_path) if isfile(join(img_path, f))]\n",
    "        X = X + [os.path.join(img_path, f) for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        y = y + [d for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        #print(d)\n",
    "\n",
    "    data_dir = Path(base_img_path)\n",
    "    image_count = len(list(data_dir.glob('*/*.*')))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "\n",
    "def preprocess_image(filename, label):\n",
    "    \"\"\"Function that returns a tuple of normalized image array and labels array.\n",
    "    Args:\n",
    "        filename: string representing path to image\n",
    "        label: 0/1 one-dimensional array of size N_LABELS\n",
    "    \"\"\"\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=CHANNELS)\n",
    "    image_resized = tf.image.resize_with_pad(image_decoded, IMG_HEIGHT, IMG_WIDTH, antialias=False)\n",
    "    image_normalized = image_resized / 255.0\n",
    "    \n",
    "    return image_normalized, label\n",
    "\n",
    "\n",
    "def create_dataset(filenames, labels, is_training=True):\n",
    "    \"\"\"Load and parse dataset.\n",
    "    Args:\n",
    "        filenames: list of image paths\n",
    "        labels: numpy array of shape (BATCH_SIZE, N_LABELS)\n",
    "        is_training: boolean to indicate training mode\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a first dataset of file paths and labels\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n",
    "    # Parse and preprocess observations in parallel\n",
    "    dataset = dataset.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    \n",
    "    if is_training == True:\n",
    "        #dataset = dataset.take(BATCH_SIZE)\n",
    "        # This is a small dataset, only load it once, and keep it in memory.\n",
    "        #dataset = dataset.cache()\n",
    "        #dataset = dataset.repeat()\n",
    "        # Shuffle the data each buffer size\n",
    "        dataset = dataset.shuffle(buffer_size=SHUFFLE_BUFFER_SIZE)\n",
    "        #dataset = dataset.repeat()\n",
    "        \n",
    "    # Batch the data for multiple steps    \n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "    # Fetch batches in the background while the model is training.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "    \n",
    "def visualize_image(original, augmented):\n",
    "    org_img = tf.keras.utils.array_to_img(original)\n",
    "    fig = plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img)\n",
    "\n",
    "    aug_img = tf.keras.utils.array_to_img(augmented)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Augmented image')\n",
    "    plt.imshow(aug_img)\n",
    "\n",
    "\n",
    "def predict_new_image(img_file):    \n",
    "    #img = tf.keras.utils.load_img(\n",
    "    #    img_file, target_size=(IMG_HEIGHT, IMG_WIDTH), keep_aspect_ratio=True\n",
    "    #)\n",
    "    img = tf.keras.utils.load_img(\n",
    "        img_file, target_size=None, keep_aspect_ratio=True\n",
    "    )\n",
    "\n",
    "    img_array = tf.keras.utils.img_to_array(img)\n",
    "    img_array = tf.keras.preprocessing.image.smart_resize(img_array, size=(IMG_HEIGHT, IMG_WIDTH))\n",
    "    img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "    image_normalized = img_array / 255.0  # tf.image.per_image_standardization(img_array)\n",
    "\n",
    "    saved_model = load_model(model_file)\n",
    "\n",
    "    predictions = model.predict(image_normalized)\n",
    "    score = tf.nn.softmax(predictions[0])\n",
    "    #score = tf.sigmoid(predictions[0])\n",
    "    #score = tf.nn.sigmoid_cross_entropy_with_logits(labels=class_names, logits=predictions[0])\n",
    "    #score = tf.math.sigmoid(predictions[0])\n",
    "    #score = tf.tanh(predictions[0])\n",
    "\n",
    "    #model.evaluate(img_array)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n",
    "\n",
    "    #print(predictions[0])\n",
    "\n",
    "    print(\n",
    "        \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "        .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    "    )\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arowana', 'betta', 'goldfish', 'luohan']\n",
      "0. arowana\n",
      "1. betta\n",
      "2. goldfish\n",
      "3. luohan\n",
      "(3297, 4)\n",
      "(825, 4)\n"
     ]
    }
   ],
   "source": [
    "X, Y = readImagesFromDir()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=SEED)\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "lb.fit(y_train)\n",
    "class_names = lb.classes_\n",
    "# Loop over all labels and show them    \n",
    "N_LABELS = len(class_names)\n",
    "for (i, label) in enumerate(class_names):\n",
    "    print(\"{}. {}\".format(i, label))\n",
    "\n",
    "# transform the targets of the training and test sets\n",
    "y_train_bin = lb.transform(y_train)\n",
    "y_val_bin = lb.transform(y_val)\n",
    "\n",
    "print(y_train_bin.shape)\n",
    "print(y_val_bin.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset/betta/betta0194.jpg [0 1 0 0]\n",
      "dataset/arowana/Arrowana10533.jpg [1 0 0 0]\n",
      "dataset/goldfish/Goldfish1515.jpg [0 0 1 0]\n",
      "dataset/arowana/Arrowana10911.jpg [1 0 0 0]\n",
      "dataset/betta/betta0357.jpg [0 1 0 0]\n",
      "dataset/luohan/FHCichlid100114.jpg [0 0 0 1]\n",
      "dataset/goldfish/Goldfish1548.jpg [0 0 1 0]\n",
      "dataset/luohan/FHCichlid100525.jpg [0 0 0 1]\n",
      "dataset/goldfish/Goldfish1743.jpg [0 0 1 0]\n",
      "dataset/luohan/FHCichlid100096.jpg [0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "# Print examples of fish and their binary targets\n",
    "for i in range(10):\n",
    "    print(X_train[len(X_train)-1 - i], y_train_bin[len(y_train_bin)-1 - i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = []\n",
    "val_ds = []\n",
    "\n",
    "for m in memebers_param:\n",
    "    IMG_HEIGHT = m[\"img_height\"]\n",
    "    IMG_WIDTH = m[\"img_width\"]\n",
    "    train_ds.append(create_dataset(X_train, y_train_bin))\n",
    "    val_ds.append(create_dataset(X_val, y_val_bin, is_training=False))\n",
    "\n",
    "\n",
    "# for images, labels in train_ds.take(1):\n",
    "#     print(\"Shape of features array:\", images.numpy().shape)\n",
    "#     print(\"Shape of labels array:\", labels.numpy().shape)\n",
    "#     #plt.imshow(f.numpy().astype(\"uint8\"))\n",
    "#     for i in range(5):\n",
    "#         ax = plt.subplot(2, 3, i + 1)\n",
    "#         img = tf.keras.utils.array_to_img(images[i])\n",
    "#         plt.imshow(img)\n",
    "#         plt.title(class_names[np.argmax(labels[i])])\n",
    "#         plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8122,
     "status": "ok",
     "timestamp": 1653206045624,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "gRdnqYDSaoMc",
    "outputId": "417b2730-29c8-4554-b302-5715ef394613"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">loaded ./models/medium_224_20_64_1_2022-09-25_19-33-18.hdf5\n",
      ">loaded ./models/arowana_softmax_128_50_64_1_2022-09-25_22-34-38.hdf5\n",
      ">loaded ./models/betta_softmax_128_50_64_1_2022-09-26_17-51-39.hdf5\n",
      ">loaded ./models/goldfish_softmax_128_50_64_1_2022-09-25_22-37-40.hdf5\n",
      ">loaded ./models/luohan_softmax_128_50_64_1_2022-09-25_22-42-17.hdf5\n",
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " ensemble_0conv2d_495_input (In  [(None, 160, 224, 3  0          []                               \n",
      " putLayer)                      )]                                                                \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_495 (Conv2D)  (None, 160, 224, 32  2432        ['ensemble_0conv2d_495_input[0][0\n",
      "                                )                                ]']                              \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_493 (M  (None, 80, 112, 32)  0          ['ensemble_0conv2d_495[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_496 (Conv2D)  (None, 80, 112, 64)  51264       ['ensemble_0max_pooling2d_493[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_494 (M  (None, 40, 56, 64)  0           ['ensemble_0conv2d_496[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_497 (Conv2D)  (None, 40, 56, 128)  204928      ['ensemble_0max_pooling2d_494[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_495 (M  (None, 20, 28, 128)  0          ['ensemble_0conv2d_497[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_498 (Conv2D)  (None, 20, 28, 256)  524544      ['ensemble_0max_pooling2d_495[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_496 (M  (None, 10, 14, 256)  0          ['ensemble_0conv2d_498[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_499 (Conv2D)  (None, 10, 14, 512)  2097664     ['ensemble_0max_pooling2d_496[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_497 (M  (None, 5, 7, 512)   0           ['ensemble_0conv2d_499[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0conv2d_500 (Conv2D)  (None, 5, 7, 768)    3539712     ['ensemble_0max_pooling2d_497[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_0max_pooling2d_498 (M  (None, 2, 3, 768)   0           ['ensemble_0conv2d_500[0][0]']   \n",
      " axPooling2D)                                                                                     \n",
      "                                                                                                  \n",
      " ensemble_0dropout_54 (Dropout)  (None, 2, 3, 768)   0           ['ensemble_0max_pooling2d_498[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " ensemble_1conv2d_input (InputL  [(None, 128, 128, 3  0          []                               \n",
      " ayer)                          )]                                                                \n",
      "                                                                                                  \n",
      " ensemble_2conv2d_input (InputL  [(None, 128, 128, 3  0          []                               \n",
      " ayer)                          )]                                                                \n",
      "                                                                                                  \n",
      " ensemble_3conv2d_input (InputL  [(None, 128, 128, 3  0          []                               \n",
      " ayer)                          )]                                                                \n",
      "                                                                                                  \n",
      " ensemble_4conv2d_input (InputL  [(None, 128, 128, 3  0          []                               \n",
      " ayer)                          )]                                                                \n",
      "                                                                                                  \n",
      " ensemble_0flatten_98 (Flatten)  (None, 4608)        0           ['ensemble_0dropout_54[0][0]']   \n",
      "                                                                                                  \n",
      " ensemble_1conv2d (Conv2D)      (None, 128, 128, 32  896         ['ensemble_1conv2d_input[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_2conv2d (Conv2D)      (None, 128, 128, 32  896         ['ensemble_2conv2d_input[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_3conv2d (Conv2D)      (None, 128, 128, 32  896         ['ensemble_3conv2d_input[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_4conv2d (Conv2D)      (None, 128, 128, 32  896         ['ensemble_4conv2d_input[0][0]'] \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " ensemble_0dense_572 (Dense)    (None, 768)          3539712     ['ensemble_0flatten_98[0][0]']   \n",
      "                                                                                                  \n",
      " ensemble_1max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_1conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_2max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_2conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_3max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_3conv2d[0][0]']       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_4max_pooling2d (MaxPo  (None, 64, 64, 32)  0           ['ensemble_4conv2d[0][0]']       \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " ensemble_0dense_573 (Dense)    (None, 512)          393728      ['ensemble_0dense_572[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_1max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_2conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_2max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_3conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_3max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_4conv2d_1 (Conv2D)    (None, 64, 64, 16)   4624        ['ensemble_4max_pooling2d[0][0]']\n",
      "                                                                                                  \n",
      " ensemble_0dense_574 (Dense)    (None, 256)          131328      ['ensemble_0dense_573[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_1conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_2max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_2conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_3max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_3conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_4max_pooling2d_1 (Max  (None, 32, 32, 16)  0           ['ensemble_4conv2d_1[0][0]']     \n",
      " Pooling2D)                                                                                       \n",
      "                                                                                                  \n",
      " ensemble_0dense_575 (Dense)    (None, 64)           16448       ['ensemble_0dense_574[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1dropout (Dropout)    (None, 32, 32, 16)   0           ['ensemble_1max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_2dropout (Dropout)    (None, 32, 32, 16)   0           ['ensemble_2max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_3dropout (Dropout)    (None, 32, 32, 16)   0           ['ensemble_3max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_4dropout (Dropout)    (None, 32, 32, 16)   0           ['ensemble_4max_pooling2d_1[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " ensemble_0dense_576 (Dense)    (None, 32)           2080        ['ensemble_0dense_575[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1flatten (Flatten)    (None, 16384)        0           ['ensemble_1dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_2flatten (Flatten)    (None, 16384)        0           ['ensemble_2dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_3flatten (Flatten)    (None, 16384)        0           ['ensemble_3dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_4flatten (Flatten)    (None, 16384)        0           ['ensemble_4dropout[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_0dense_577 (Dense)    (None, 16)           528         ['ensemble_0dense_576[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1dense (Dense)        (None, 10)           163850      ['ensemble_1flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_2dense (Dense)        (None, 10)           163850      ['ensemble_2flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_3dense (Dense)        (None, 10)           163850      ['ensemble_3flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_4dense (Dense)        (None, 10)           163850      ['ensemble_4flatten[0][0]']      \n",
      "                                                                                                  \n",
      " ensemble_0dense_578 (Dense)    (None, 4)            68          ['ensemble_0dense_577[0][0]']    \n",
      "                                                                                                  \n",
      " ensemble_1dense_1 (Dense)      (None, 2)            22          ['ensemble_1dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_2dense_1 (Dense)      (None, 2)            22          ['ensemble_2dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_3dense_1 (Dense)      (None, 2)            22          ['ensemble_3dense[0][0]']        \n",
      "                                                                                                  \n",
      " ensemble_4dense_1 (Dense)      (None, 2)            22          ['ensemble_4dense[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_3 (Concatenate)    (None, 12)           0           ['ensemble_0dense_578[0][0]',    \n",
      "                                                                  'ensemble_1dense_1[0][0]',      \n",
      "                                                                  'ensemble_2dense_1[0][0]',      \n",
      "                                                                  'ensemble_3dense_1[0][0]',      \n",
      "                                                                  'ensemble_4dense_1[0][0]']      \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 12)           156         ['concatenate_3[0][0]']          \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 4)            52          ['dense_6[0][0]']                \n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================================================================\n",
      "Total params: 11,182,212\n",
      "Trainable params: 208\n",
      "Non-trainable params: 11,182,004\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def add_prefix(model, prefix: str, custom_objects=None):\n",
    "    config = model.get_config()\n",
    "    new_to_old = {}\n",
    "    for layer in config['layers']:\n",
    "#         print(layer)\n",
    "        new_name = prefix + layer['config']['name']\n",
    "        new_to_old[new_name] = layer['config']['name']\n",
    "#         layer['name'] = new_name\n",
    "        layer['config']['name'] = new_name\n",
    "\n",
    "    new_model = tf.keras.Sequential().from_config(config, custom_objects)\n",
    "    \n",
    "    for layer in new_model.layers:\n",
    "        layer.set_weights(model.get_layer(new_to_old[layer.name]).get_weights())\n",
    "    \n",
    "    return new_model\n",
    "\n",
    "# load models from file\n",
    "def load_all_models(memebers_param):\n",
    "    all_models = list()\n",
    "    prefix = \"ensemble_\"\n",
    "    c = 0\n",
    "    for m in memebers_param:\n",
    "        # define filename for this ensemble\n",
    "        filename = m[\"filePath\"]\n",
    "        # load model from file\n",
    "        model = load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(add_prefix(model,prefix+str(c)))\n",
    "        c+=1\n",
    "        print('>loaded %s' % filename)\n",
    "    return all_models\n",
    " \n",
    "# define stacked model from multiple member input models\n",
    "def define_stacked_model(members):\n",
    "    # update all layers in all models to not be trainable\n",
    "    for i in range(len(members)):\n",
    "        model = members[i]\n",
    "        for layer in model.layers:\n",
    "            # make not trainable\n",
    "            layer.trainable = False\n",
    "    # define multi-headed input\n",
    "    ensemble_visible = [model.input for model in members]\n",
    "    # concatenate merge output from each model\n",
    "    ensemble_outputs = [model.output for model in members]\n",
    "    merge = concatenate(ensemble_outputs)\n",
    "    hidden = Dense(12, activation='relu')(merge)\n",
    "    output = Dense(N_LABELS, activation='softmax')(hidden)\n",
    "    model = Model(inputs=ensemble_visible, outputs=output)\n",
    "\n",
    "    # compile\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optmz, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = define_stacked_model(load_all_models(memebers_param))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 212
    },
    "executionInfo": {
     "elapsed": 760,
     "status": "ok",
     "timestamp": 1653206059750,
     "user": {
      "displayName": "Fred Zhu",
      "userId": "17012624792468068711"
     },
     "user_tz": -480
    },
    "id": "oiFHNVtKaoMc",
    "outputId": "548eef54-ba28-4e79-f3d5-9e156fc04d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to model: models/arbitrator_softmax_50_32_1_2022-10-07_17-36-19.hdf5\n",
      "Path to log:   models/arbitrator_softmax_50_32_1_2022-10-07_17-36-19.csv\n"
     ]
    }
   ],
   "source": [
    "                                                                                # Step 1\n",
    "modelname       = modelname+\"_\"+str(datetime.datetime.now())[:-7].replace(' ','_').replace(\":\",'-')\n",
    "folderpath      = 'models/'\n",
    "model_json      = folderpath + modelname + \".json\"\n",
    "with open(model_json, \"w\") as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "model_file      = folderpath + modelname + \".hdf5\"\n",
    "checkpoint      = ModelCheckpoint(model_file, \n",
    "                                  monitor='val_accuracy', \n",
    "                                  verbose=0, \n",
    "                                  save_best_only=True, \n",
    "                                  mode='max')\n",
    "\n",
    "csv_logger      = CSVLogger(folderpath+modelname +'.csv')                       # Step 2\n",
    "callbacks_list  = [checkpoint,csv_logger]                                       # Step 3\n",
    "\n",
    "print(\"Path to model:\", model_file)\n",
    "print(\"Path to log:  \", folderpath+modelname+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>, <PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>, <PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>, <PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>, <PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      3\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# Training data and label\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# Validation data and label\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                       \u001b[49m\u001b[38;5;66;43;03m# The amount of epochs to be trained\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                   \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m          \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m                     \u001b[49m\u001b[38;5;66;43;03m# To shuffle the training data\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks_list\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# Callbacks to execute the checkpoints\u001b[39;00m\n\u001b[0;32m     11\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     12\u001b[0m duration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(((end \u001b[38;5;241m-\u001b[39m start)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\data_adapter.py:1579\u001b[0m, in \u001b[0;36munpack_x_y_sample_weight\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1577\u001b[0m   error_msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData is expected to be in format `x`, `(x,)`, `(x, y)`, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1578\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `(x, y, sample_weight)`, found: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(data)\n\u001b[1;32m-> 1579\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Data is expected to be in format `x`, `(x,)`, `(x, y)`, or `(x, y, sample_weight)`, found: (<PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>, <PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>, <PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>, <PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>, <PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 4), dtype=tf.int32, name=None))>)"
     ]
    }
   ],
   "source": [
    "import time as time\n",
    "\n",
    "start = time.time()\n",
    "history = model.fit(train_ds,                            # Training data and label\n",
    "          validation_data=val_ds,   # Validation data and label\n",
    "          epochs=EPOCHS,                       # The amount of epochs to be trained\n",
    "          batch_size=BATCH_SIZE,                   \n",
    "          shuffle=True,                     # To shuffle the training data\n",
    "          callbacks=callbacks_list)         # Callbacks to execute the checkpoints\n",
    "\n",
    "end = time.time()\n",
    "duration = round(((end - start)/60), 2)\n",
    "print(\"duration = \", duration, \" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YSN4w8WeaoMd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to plot: models/arbitrator_softmax_50_32_1_2022-10-06_20-44-16_plot.png\n"
     ]
    }
   ],
   "source": [
    "plotpath  = folderpath+modelname+'_plot.png'\n",
    "plot_model(model, \n",
    "           to_file=plotpath, \n",
    "           show_shapes=True, \n",
    "           show_layer_names=False,\n",
    "           rankdir='TB')\n",
    "print(\"Path to plot:\", plotpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at models/arbitrator_softmax_50_32_1_2022-10-06_20-44-16.hdf5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m modelGo \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m predicts    \u001b[38;5;241m=\u001b[39m modelGo\u001b[38;5;241m.\u001b[39mpredict(val_ds)                                            \u001b[38;5;66;03m# Step 2\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction completes.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\saving\\save.py:206\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    205\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    208\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(filepath_str, \u001b[38;5;28mcompile\u001b[39m, options)\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at models/arbitrator_softmax_50_32_1_2022-10-06_20-44-16.hdf5"
     ]
    }
   ],
   "source": [
    "                                                                                # Step 1\n",
    "modelGo = load_model(model_file)\n",
    "\n",
    "predicts    = modelGo.predict(val_ds)                                            # Step 2\n",
    "print(\"Prediction completes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Step 1\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Step 2\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m predout     \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mpredicts\u001b[49m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m testout     \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_val_bin,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m testScores  \u001b[38;5;241m=\u001b[39m metrics\u001b[38;5;241m.\u001b[39maccuracy_score(testout,predout)                           \u001b[38;5;66;03m# Step 3\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predicts' is not defined"
     ]
    }
   ],
   "source": [
    "                                                                                # Step 1\n",
    "                                                                                # Step 2\n",
    "predout     = np.argmax(predicts,axis=1)\n",
    "testout     = np.argmax(y_val_bin,axis=1)\n",
    "\n",
    "testScores  = metrics.accuracy_score(testout,predout)                           # Step 3\n",
    "\n",
    "                                                                                # Step 4\n",
    "print(\"Best accuracy (on testing dataset): %.2f%%\" % (testScores*100))\n",
    "print(metrics.classification_report(testout,\n",
    "                                    predout,\n",
    "                                    target_names=class_names,\n",
    "                                    digits=4))\n",
    "\n",
    "report = metrics.classification_report(testout,\n",
    "                                    predout,\n",
    "                                    target_names=class_names,\n",
    "                                    digits=4,\n",
    "                                      output_dict=True)\n",
    "\n",
    "df = pd.DataFrame(report).transpose()\n",
    "df.to_csv(folderpath+modelname+'_report.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "D5 StackingEnsembleNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
